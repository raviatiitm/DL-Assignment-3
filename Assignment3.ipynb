{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/hin_train.csv\"\n",
        "val_path = \"/content/hin_valid.csv\"\n",
        "test_path = \"/content/hin_test.csv\""
      ],
      "metadata": {
        "id": "XlgfOa8ETfed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bejOPlbHmFyY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import zipfile\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createVocab(path):\n",
        "        # d = pd.read_csv(path,sep=\"\\t\",header=None)\n",
        "        # d = d.dropna()\n",
        "        # print(d.head())\n",
        "\n",
        "        file = open(path)\n",
        "        dataset = csv.reader(file, delimiter = \",\")\n",
        "\n",
        "        hindi = []\n",
        "        english = []\n",
        "\n",
        "        #get the words in a list\n",
        "\n",
        "        for data in dataset:\n",
        "          english.append(data[0])\n",
        "          hindi.append(data[1])\n",
        "\n",
        "        # print(english)\n",
        "        # print(hindi)\n",
        "\n",
        "        #append start and end characters to output - kannada\n",
        "        for i in range(len(hindi)):\n",
        "            hindi[i] = \"\\t\" + hindi[i] +\"\\n\"\n",
        "\n",
        "        return np.array(hindi), np.array(english)"
      ],
      "metadata": {
        "id": "oMbN4wDcQ9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#createVocab(train_path)"
      ],
      "metadata": {
        "id": "7cju_ex1JHQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getChar(data):\n",
        "      data_char = set() #to store the the different characters present in data\n",
        "      data_char.add(\" \")\n",
        "      for word in data:\n",
        "        for char in word:\n",
        "          if char not in data_char:\n",
        "            data_char.add(char)\n",
        "\n",
        "      #sort the characters in dataset\n",
        "      data_char = sorted(list(data_char))\n",
        "\n",
        "      #number of characters in the set\n",
        "      num_tokens = len(data_char)\n",
        "\n",
        "      #get the max length of the words\n",
        "      max_len = max([len(word) for word in data])\n",
        "\n",
        "      #return set of all characters in data\n",
        "      return data_char, num_tokens, max_len "
      ],
      "metadata": {
        "id": "lcJTgo79REyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getData(english, hindi, max_enc_len, max_dec_len, num_decoder_tokens, input_token_index, target_token_index):\n",
        "      #initializing with 0s for max_length\n",
        "      encoder_input_data = np.zeros((len(english), max_enc_len), dtype=\"float32\") #(51200,24)\n",
        "      decoder_input_data = np.zeros((len(english), max_dec_len), dtype=\"float32\") #(51200,22)\n",
        "      decoder_target_data = np.zeros((len(english), max_dec_len, len(target_token_index)), dtype=\"float32\") #(51200,22,67)\n",
        "\n",
        "      #populating indices for characters that exist\n",
        "      for i, (english, hindi) in enumerate(zip(english, hindi)):\n",
        "          for t, char in enumerate(english):\n",
        "              encoder_input_data[i, t] = input_token_index[char]\n",
        "          \n",
        "          for t, char in enumerate(hindi):\n",
        "              decoder_input_data[i, t] = target_token_index[char]\n",
        "              if t > 0:\n",
        "                  # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "                  decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "                  \n",
        "          decoder_input_data[i, t+1:] = target_token_index[' ']\n",
        "          decoder_target_data[i, t :, target_token_index[' ']] = 1.0\n",
        "\n",
        "      return encoder_input_data, decoder_input_data, decoder_target_data"
      ],
      "metadata": {
        "id": "UrYHT-LQRyJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createDictionary(input_tokens, target_tokens):\n",
        "\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_tokens)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_tokens)])\n",
        "\n",
        "    reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "    reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "    return input_token_index,target_token_index,reverse_input_char_index,reverse_target_char_index"
      ],
      "metadata": {
        "id": "jJLxzK6uQSHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "        #get the training words as an array from train directory\n",
        "        train_hindi_words, train_english_words =createVocab(train_path)\n",
        "        #get validation words\n",
        "        val_hindi_words, val_english_words =createVocab(val_path)\n",
        "\n",
        "        test_hindi_words, test_english_words =createVocab(test_path)\n",
        "  \n",
        "        #get the characters from train and val dataset\n",
        "        train_eng_characters, train_num_encoder_tokens, train_max_enc_len = getChar(train_english_words)\n",
        "        train_hin_characters, train_num_decoder_tokens, train_max_dec_len = getChar(train_hindi_words)\n",
        "\n",
        "        val_eng_characters, val_num_encoder_tokens, val_max_enc_len = getChar(val_english_words)\n",
        "        val_hin_characters, val_num_decoder_tokens, val_max_dec_len = getChar(val_hindi_words)\n",
        "\n",
        "        test_eng_characters, test_num_encoder_tokens, test_max_enc_len = getChar(train_english_words)\n",
        "        test_hin_characters, test_num_decoder_tokens, test_max_dec_len = getChar(test_hindi_words)\n",
        "\n",
        "        #take the largest number of tokens and max_length of words on both encoder and decoder\n",
        "        num_encoder_tokens = max(val_num_encoder_tokens, train_num_encoder_tokens,test_num_encoder_tokens)\n",
        "        num_decoder_tokens = max(val_num_decoder_tokens, train_num_decoder_tokens,test_num_decoder_tokens)\n",
        "\n",
        "        max_enc_len = max(train_max_enc_len, val_max_enc_len,test_max_enc_len)\n",
        "        max_dec_len = max(train_max_dec_len, val_max_dec_len,test_max_dec_len)\n",
        "        # print(max_enc_len)\n",
        "        # print(max_dec_len)\n",
        "\n",
        "        input_hin_characters=set()\n",
        "        for char in train_hin_characters:\n",
        "          input_hin_characters.add(char)\n",
        "        \n",
        "        for char in val_hin_characters:\n",
        "          input_hin_characters.add(char)\n",
        "        \n",
        "        for char in test_hin_characters:\n",
        "          input_hin_characters.add(char)\n",
        "        \n",
        "        # print(len(input_hin_characters))\n",
        "        # print(input_hin_characters)\n",
        "        #making a dictionary and reverse dictionary to map the characters with the indices and indices to characters\n",
        "        input_token_index,target_token_index,reverse_input_char_index,reverse_target_char_index=createDictionary(train_eng_characters,list(input_hin_characters)) \n",
        "        \n",
        "        # print(len(input_token_index))\n",
        "        #print(len(target_token_index))\n",
        "\n",
        "        train_encoder_input_data, train_decoder_input_data, train_decoder_target_data = getData(train_english_words, train_hindi_words, max_enc_len, max_dec_len, num_decoder_tokens, input_token_index, target_token_index)\n",
        "        val_encoder_input_data, val_decoder_input_data, val_decoder_target_data = getData(val_english_words, val_hindi_words, max_enc_len, max_dec_len, num_decoder_tokens, input_token_index, target_token_index)\n",
        "\n",
        "        train_encoder_input_data=torch.from_numpy(train_encoder_input_data).to(device).long()\n",
        "\n",
        "        train_decoder_input_data=torch.from_numpy(train_decoder_input_data).to(device).long()\n",
        "        train_decoder_target_data=torch.from_numpy(train_decoder_target_data).to(device).long()\n",
        "\n",
        "        # print(train_encoder_input_data.size())\n",
        "        # print(train_decoder_input_data.size())\n",
        "        # print(train_decoder_target_data.size())\n",
        "        return train_encoder_input_data,train_decoder_input_data,train_decoder_target_data,max_enc_len,max_dec_len,len(input_token_index),len(target_token_index)"
      ],
      "metadata": {
        "id": "Dy4a5h_TmaT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_enc_len=24\n",
        "# max_dec_len=22\n",
        "# input_token_index=27\n",
        "# target_token_index=67"
      ],
      "metadata": {
        "id": "kB6h0Dd_2v2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main()"
      ],
      "metadata": {
        "id": "9t65LlxjRnRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AFRC5x02mWFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,train_eng_characters,embedding_size,hidden_size,input_token_index_len,no_of_layers):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.train_eng_characters=train_eng_characters\n",
        "        self.hidden_size=hidden_size\n",
        "        self.input_token_index_len=input_token_index_len\n",
        "        self.no_of_layers = no_of_layers\n",
        "        self.encoder_embedding = nn.Embedding(self.input_token_index_len,self.embedding_size).to(device)\n",
        "        self.encoder_rnn = nn.GRU(self.embedding_size,hidden_size,self.no_of_layers,batch_first = True).to(device)\n",
        "    def forward(self,input,hidden):\n",
        "        enc_embedd= self.encoder_embedding(input)\n",
        "        out,enc_hidden = self.encoder_rnn(enc_embedd,hidden)\n",
        "        return out,enc_hidden"
      ],
      "metadata": {
        "id": "oY6JUAaRfRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,train_hin_characters,embedding_size,hidden_size,target_token_index_len,no_of_layers):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.train_hin_characters=train_hin_characters\n",
        "        self.hidden_size=hidden_size\n",
        "        self.no_of_layers = no_of_layers\n",
        "        self.target_token_index_len=target_token_index_len\n",
        "        self.decoder_embedding = nn.Embedding(self.target_token_index_len,self.embedding_size).to(device)\n",
        "        self.decoder_rnn = nn.GRU(self.embedding_size,self.hidden_size,self.no_of_layers,batch_first = True).to(device)\n",
        "        self.linear = nn.Linear(self.hidden_size,self.target_token_index_len,bias=True).to(device)\n",
        "        # dim = 2 \n",
        "        self.softmax = nn.Softmax(dim = 2).to(device)\n",
        "    def forward(self,input,hidden):\n",
        "        dec_embedd = self.decoder_embedding(input)\n",
        "        out,dec_hidden = self.decoder_rnn(dec_embedd,hidden)\n",
        "        output1 = self.linear(out)\n",
        "        return output1,dec_hidden\n",
        "        output2 = self.softmax(output1)\n",
        "        return output2,hidden1"
      ],
      "metadata": {
        "id": "b0xpFmOgDVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "\n",
        "embedding_size = 256\n",
        "\n",
        "no_of_layers = 2\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "batchsize = 1024"
      ],
      "metadata": {
        "id": "VCryWTrGtJhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(target,predictions,flag):\n",
        "    total = 0\n",
        "    for x in range(len(target)):\n",
        "        if(torch.equal(target[x],predictions[x])):\n",
        "            total += 1\n",
        "    return total"
      ],
      "metadata": {
        "id": "4TkcsGWPU5EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    encoder_input_data,decoder_input_data,decoder_target_data,max_enc_len,max_dec_len,input_token_index_len,target_token_index_len=main()\n",
        "    encoder = Encoder(encoder_input_data,embedding_size,hidden_size,input_token_index_len,no_of_layers).to(device)\n",
        "    decoder = Decoder(decoder_input_data,embedding_size,hidden_size,target_token_index_len,no_of_layers).to(device)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(),lr = 0.001)\n",
        "    decoder_optimizer  = optim.Adam(decoder.parameters(),lr = 0.001)\n",
        "    for _ in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for x in range(0,len(encoder_input_data),batchsize):\n",
        "            loss = 0\n",
        "            input_tensor = encoder_input_data[x:x+batchsize].to(device)\n",
        "            hidden_input0 = torch.zeros(no_of_layers,batchsize,hidden_size).to(device)\n",
        "            if(input_tensor.size()[0] < batchsize):\n",
        "                break\n",
        "            output,hidden = encoder.forward(input_tensor,hidden_input0)\n",
        "            input2 = decoder_input_data[x:x+batchsize,0].to(device).resize(batchsize,1)\n",
        "            #input2 = (torch.tensor(input2)).view(batchsize,1).to(device)\n",
        "            hidden1 = hidden\n",
        "            predicted = []\n",
        "            predictions = []\n",
        "            for i in range(22):\n",
        "                output1,hidden1 = decoder.forward(input2,hidden1)\n",
        "                #print(output1.size()) #(1024,1,68)\n",
        "                #predicted.append(output1)\n",
        "                #print(len(predicted[0]))\n",
        "                output2 = decoder.softmax(output1)\n",
        "                predicted.append(output2)\n",
        "                #print(output2.size()) #(1024,1,68)\n",
        "                output3 = torch.argmax(output2,dim = 2)\n",
        "                predictions.append(output3)\n",
        "                input2 = output3\n",
        "            \n",
        "            predicted = torch.cat(tuple(x for x in predicted),dim =1).to(device).resize(max_dec_len*batchsize,target_token_index_len)\n",
        "            predictions = torch.cat(tuple(x for x in predictions),dim =1).to(device)\n",
        "            total_acc += accuracy(decoder_target_data[x:x+batchsize].to(device),predictions,x)\n",
        "            loss  = nn.CrossEntropyLoss(reduction = 'sum')(predicted,decoder_input_data[x:x+batchsize].reshape(-1).to(device))\n",
        "            with torch.no_grad():\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            encoder_optimizer.zero_grad()\n",
        "            decoder_optimizer.zero_grad()\n",
        "            loss.backward(retain_graph = True)\n",
        "            torch.nn.utils.clip_grad_norm_(encoder.parameters(),max_norm = 1)\n",
        "            torch.nn.utils.clip_grad_norm_(decoder.parameters(),max_norm = 1)\n",
        "            encoder_optimizer.step()\n",
        "            decoder_optimizer.step()\n",
        "        print(total_loss/(51200*22))\n",
        "        print(total_acc)\n",
        "            "
      ],
      "metadata": {
        "id": "87usnvHBDVq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "gprurwqvukte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wkxptRmm1HzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}